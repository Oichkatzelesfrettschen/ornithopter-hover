name: Build, Test, and Verify

on:
  push:
    branches: [ main, develop, 'copilot/**' ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:  # Allow manual triggering
  schedule:
    - cron: '0 0 * * 0'  # Weekly verification on Sunday at midnight

env:
  TLA_VERSION: "1.7.1"
  PLATFORMIO_CORE_DIR: .platformio
  CACHE_VERSION: v1  # Increment to invalidate all caches

# Cancel in-progress runs when a new push is made
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # Code quality checks
  lint:
    name: Code Quality Analysis
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0  # Fetch all history for better analysis
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Cache pip packages
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-
            ${{ runner.os }}-pip-
      
      - name: Install linting tools
        run: |
          pip install --upgrade pip
          pip install pylint flake8 cpplint cppcheck
      
      - name: Lint Python code
        run: |
          echo "=== Python Linting ==="
          flake8 verification/ --max-line-length=100 --ignore=E501,W503 || true
          pylint verification/*.py --disable=C0111,C0103,R0913,R0914 --max-line-length=100 || true
        continue-on-error: true
      
      - name: Lint C++ code
        run: |
          echo "=== C++ Linting ==="
          find src/ tests/ -name "*.h" -o -name "*.cpp" | xargs cpplint --filter=-whitespace,-legal --linelength=100 || true
        continue-on-error: true
      
      - name: Static analysis with cppcheck
        run: |
          echo "=== C++ Static Analysis ==="
          cppcheck --enable=warning,style,performance,portability --suppress=missingIncludeSystem \
            --inline-suppr src/ tests/ 2>&1 | tee cppcheck-report.txt || true
        continue-on-error: true
      
      - name: Upload analysis results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: code-quality-reports
          path: |
            cppcheck-report.txt
  
  # Security scanning
  security:
    name: Security Scanning
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install safety for Python dependency scanning
        run: |
          pip install safety bandit
      
      - name: Scan Python dependencies for vulnerabilities
        run: |
          echo "=== Python Dependency Security Scan ==="
          pip freeze > requirements-frozen.txt
          safety check --file requirements-frozen.txt --continue-on-error || true
        continue-on-error: true
      
      - name: Scan Python code with Bandit
        run: |
          echo "=== Python Security Analysis ==="
          bandit -r verification/ -f txt -o bandit-report.txt || true
        continue-on-error: true
      
      - name: Upload security reports
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: security-reports
          path: |
            bandit-report.txt
            requirements-frozen.txt
  
  build:
    name: Build Firmware
    runs-on: ubuntu-latest
    needs: [lint]  # Run after code quality checks
    strategy:
      fail-fast: false  # Continue other builds even if one fails
      matrix:
        environment: [arduino_nano, teensy40, esp32]
        include:
          - environment: arduino_nano
            platform: atmelavr
            optimization: size
          - environment: teensy40
            platform: teensy
            optimization: speed
          - environment: esp32
            platform: espressif32
            optimization: balanced
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Cache PlatformIO
        uses: actions/cache@v3
        with:
          path: |
            ~/.platformio
            .pio
          key: ${{ runner.os }}-pio-${{ env.CACHE_VERSION }}-${{ matrix.platform }}-${{ hashFiles('**/platformio.ini') }}
          restore-keys: |
            ${{ runner.os }}-pio-${{ env.CACHE_VERSION }}-${{ matrix.platform }}-
            ${{ runner.os }}-pio-${{ env.CACHE_VERSION }}-
            ${{ runner.os }}-pio-
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install PlatformIO
        run: |
          pip install --upgrade pip
          pip install platformio
      
      - name: Check PlatformIO installation
        run: |
          pio --version
          pio system info
      
      - name: Build firmware for ${{ matrix.environment }}
        run: |
          echo "Building for ${{ matrix.environment }} (optimization: ${{ matrix.optimization }})"
          platformio run -e ${{ matrix.environment }} -v
      
      - name: Check firmware size
        run: |
          echo "=== Firmware Size Report for ${{ matrix.environment }} ==="
          if [ -f .pio/build/${{ matrix.environment }}/firmware.elf ]; then
            size .pio/build/${{ matrix.environment }}/firmware.elf
          fi
      
      - name: Upload firmware artifacts
        uses: actions/upload-artifact@v3
        with:
          name: firmware-${{ matrix.environment }}
          path: |
            .pio/build/${{ matrix.environment }}/firmware.*
            .pio/build/${{ matrix.environment }}/*.map
          retention-days: 30
  
  test:
    name: Run Unit Tests
    runs-on: ubuntu-latest
    needs: [lint]  # Run after code quality checks
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Cache PlatformIO
        uses: actions/cache@v3
        with:
          path: |
            ~/.platformio
            .pio
          key: ${{ runner.os }}-pio-test-${{ env.CACHE_VERSION }}-${{ hashFiles('**/platformio.ini') }}
          restore-keys: |
            ${{ runner.os }}-pio-test-${{ env.CACHE_VERSION }}-
            ${{ runner.os }}-pio-test-
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install PlatformIO
        run: |
          pip install --upgrade pip
          pip install platformio
      
      - name: Run native tests with verbose output
        run: |
          echo "=== Running Unit Tests ==="
          platformio test -e native -v
      
      - name: Generate test coverage report
        run: |
          echo "=== Test Coverage Summary ==="
          if [ -d .pio/test ]; then
            find .pio/test -name "*.xml" -o -name "*.log" | head -5
          fi
        continue-on-error: true
      
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: test-results
          path: |
            .pio/test/
            .pio/build/native/
          retention-days: 14
  
  formal-verification:
    name: Formal Verification
    runs-on: ubuntu-latest
    needs: [lint, security]  # Run after quality and security checks
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Cache verification tools
        uses: actions/cache@v3
        with:
          path: |
            ~/.cache/pip
            tools/
          key: ${{ runner.os }}-verification-${{ env.CACHE_VERSION }}-${{ env.TLA_VERSION }}
          restore-keys: |
            ${{ runner.os }}-verification-${{ env.CACHE_VERSION }}-
            ${{ runner.os }}-verification-
      
      - name: Install Z3 and dependencies
        run: |
          pip install --upgrade pip
          pip install z3-solver pytest pytest-cov
      
      - name: Run Z3 verification with detailed output
        run: |
          echo "=== Z3 SMT Solver Verification ==="
          python verification/verify_all.py | tee z3-verification.log
      
      - name: Set up Java (for TLA+)
        uses: actions/setup-java@v3
        with:
          distribution: 'temurin'
          java-version: '11'
          cache: 'maven'
      
      - name: Download TLA+ Tools
        run: |
          mkdir -p tools
          cd tools
          if [ ! -f tla2tools.jar ]; then
            echo "Downloading TLA+ Tools version ${{ env.TLA_VERSION }}..."
            wget -q https://github.com/tlaplus/tlaplus/releases/download/v${{ env.TLA_VERSION }}/tla2tools.jar
            echo "Download complete."
          else
            echo "TLA+ Tools already cached."
          fi
          ls -lh tla2tools.jar
      
      - name: Validate TLA+ specification syntax
        run: |
          echo "=== Validating TLA+ Specification ==="
          cd specs
          java -cp ../tools/tla2tools.jar tla2sany.SANY OrnithopterController.tla
      
      - name: Run TLA+ Model Checking
        run: |
          echo "=== Running TLA+ Model Checker ==="
          cd specs
          java -Xmx4g -jar ../tools/tla2tools.jar -workers auto -cleanup OrnithopterController.tla | tee ../tla-verification.log
          echo "TLA+ verification completed"
      
      - name: Check TLA+ results
        run: |
          cd specs
          if [ -f OrnithopterController.out ]; then
            echo "=== TLA+ Output File ==="
            cat OrnithopterController.out
            if grep -qi "Error" OrnithopterController.out; then
              echo "âŒ TLA+ verification found errors!"
              exit 1
            fi
            if grep -qi "violation" OrnithopterController.out; then
              echo "âŒ TLA+ found property violations!"
              exit 1
            fi
            echo "âœ“ TLA+ verification passed successfully"
          else
            echo "âš  No TLA+ output file found"
          fi
      
      - name: Upload verification results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: verification-results
          path: |
            specs/*.out
            specs/*.log
            z3-verification.log
            tla-verification.log
          retention-days: 30
  
  documentation:
    name: Documentation Validation
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install documentation tools
        run: |
          pip install --upgrade pip
          pip install mkdocs mkdocs-material markdown-link-checker
      
      - name: Validate markdown syntax
        run: |
          echo "=== Markdown Validation ==="
          find docs -name "*.md" -type f -exec echo "Checking: {}" \; -exec head -1 {} \;
      
      - name: Check documentation links
        run: |
          echo "=== Checking Documentation Links ==="
          find docs -name "*.md" -type f -print0 | xargs -0 -I {} sh -c 'echo "Validating: {}"; grep -q "^# " {}'
        continue-on-error: true
      
      - name: Build documentation with MkDocs
        run: |
          echo "=== Building Documentation Site ==="
          echo "site_name: Ornithopter Hover" > mkdocs.yml
          echo "docs_dir: docs" >> mkdocs.yml
          echo "site_dir: site" >> mkdocs.yml
          mkdocs build --strict || true
        continue-on-error: true
      
      - name: Upload documentation
        uses: actions/upload-artifact@v3
        with:
          name: documentation
          path: |
            docs/
            site/
          retention-days: 14
  
  # Integration summary with status reporting
  summary:
    name: Integration Summary
    runs-on: ubuntu-latest
    needs: [lint, security, build, test, formal-verification, documentation]
    if: always()
    
    steps:
      - name: Check job statuses
        id: check_status
        run: |
          LINT_STATUS="${{ needs.lint.result }}"
          SECURITY_STATUS="${{ needs.security.result }}"
          BUILD_STATUS="${{ needs.build.result }}"
          TEST_STATUS="${{ needs.test.result }}"
          VERIFICATION_STATUS="${{ needs.formal-verification.result }}"
          DOCS_STATUS="${{ needs.documentation.result }}"
          
          echo "lint_status=$LINT_STATUS" >> $GITHUB_OUTPUT
          echo "security_status=$SECURITY_STATUS" >> $GITHUB_OUTPUT
          echo "build_status=$BUILD_STATUS" >> $GITHUB_OUTPUT
          echo "test_status=$TEST_STATUS" >> $GITHUB_OUTPUT
          echo "verification_status=$VERIFICATION_STATUS" >> $GITHUB_OUTPUT
          echo "docs_status=$DOCS_STATUS" >> $GITHUB_OUTPUT
      
      - name: Generate summary report
        run: |
          echo "# ðŸš€ Ornithopter CI/CD Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow Run:** #${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
          echo "**Triggered by:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Job Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Function to generate status emoji
          get_emoji() {
            case "$1" in
              success) echo "âœ…" ;;
              failure) echo "âŒ" ;;
              cancelled) echo "âš ï¸" ;;
              skipped) echo "â­ï¸" ;;
              *) echo "â“" ;;
            esac
          }
          
          echo "| Job | Status | Result |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Code Quality | $(get_emoji ${{ steps.check_status.outputs.lint_status }}) | ${{ steps.check_status.outputs.lint_status }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Security Scan | $(get_emoji ${{ steps.check_status.outputs.security_status }}) | ${{ steps.check_status.outputs.security_status }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Firmware Build | $(get_emoji ${{ steps.check_status.outputs.build_status }}) | ${{ steps.check_status.outputs.build_status }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Unit Tests | $(get_emoji ${{ steps.check_status.outputs.test_status }}) | ${{ steps.check_status.outputs.test_status }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Formal Verification | $(get_emoji ${{ steps.check_status.outputs.verification_status }}) | ${{ steps.check_status.outputs.verification_status }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Documentation | $(get_emoji ${{ steps.check_status.outputs.docs_status }}) | ${{ steps.check_status.outputs.docs_status }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Overall status
          if [[ "${{ steps.check_status.outputs.build_status }}" == "success" ]] && \
             [[ "${{ steps.check_status.outputs.test_status }}" == "success" ]] && \
             [[ "${{ steps.check_status.outputs.verification_status }}" == "success" ]]; then
            echo "## âœ… Pipeline Status: SUCCESS" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "All critical jobs completed successfully!" >> $GITHUB_STEP_SUMMARY
          else
            echo "## âŒ Pipeline Status: FAILED" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "One or more critical jobs failed. Please review the logs." >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "*Generated at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')*" >> $GITHUB_STEP_SUMMARY
